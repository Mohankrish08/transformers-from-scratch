{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "### Attention in Transformres\n",
    "\n",
    "`Q` - What I am Looking for: [sequence length * dk]\n",
    "\n",
    "`k` - what I can offer: [sequence length * dk]\n",
    "\n",
    "`v` - what I actually offer: [sequence length * dv]\n",
    "\n",
    "`L` - Length of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, d_k, d_v = 4,8,8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      ": [[ 1.15903265 -0.03436588 -0.54091411 -0.04183514  1.0294876  -1.38443887\n",
      "  -0.20643329  0.63079036]\n",
      " [ 0.28384774 -0.99402038  0.19379103  0.07773752 -0.54139472  0.61485768\n",
      "  -0.66874001 -0.61946928]\n",
      " [-0.4069266   0.21494398 -1.1802294  -0.75233679  0.07718897 -1.41885616\n",
      "   0.21152508  0.7129345 ]\n",
      " [ 0.1134854   0.8669609   0.64626725  0.42897377  0.56820766  0.60987589\n",
      "  -0.0463874   0.50168931]]\n",
      "K\n",
      ": [[ 0.45504661 -0.32470325 -0.69371571 -0.29226108  0.61103539 -0.04622356\n",
      "  -0.63832225 -0.2160977 ]\n",
      " [-0.61339223  0.52199931  0.0452046   0.16846916 -0.05370172  0.0053866\n",
      "  -1.15041188 -2.96587082]\n",
      " [ 0.25472673 -0.12084354  0.78193255 -0.32265847  0.87753825  0.55725366\n",
      "  -1.7554051   1.73041697]\n",
      " [ 0.94920364  0.30000482  0.87418704  1.02043214 -1.29928868  0.21313945\n",
      "  -0.90408828 -0.19782976]]\n",
      "V\n",
      ": [[ 0.58005958  1.18831246  0.4010658  -1.55581048  1.41179241  0.5148482\n",
      "  -0.17225128  0.41666077]\n",
      " [ 0.3933322   0.28408931 -0.87984952  0.59853989 -0.10313076 -0.00300281\n",
      "   0.23688361 -0.50214727]\n",
      " [-0.64881368  0.19596142  1.57076871  0.11652266 -0.29201105  0.57505215\n",
      "  -1.44319289 -1.18547145]\n",
      " [-0.52384454  0.4736236  -0.53012659  1.18160556  2.4767373   0.50507957\n",
      "  -0.90459399 -0.21249256]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Q\\n: {q}\")\n",
    "print(f\"K\\n: {k}\")\n",
    "print(f\"V\\n: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "We need to every word to look at every single word if it's have a high affinity towards it or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.61454566, -2.45648243,  1.4757651 , -0.99653726],\n",
       "       [ 0.49627535,  1.9678468 ,  0.28837998,  1.78158179],\n",
       "       [ 0.60732396, -2.18789169, -0.67030639, -2.85620385],\n",
       "       [-0.5633614 , -0.97738461,  2.07910641,  0.70492234]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4602380800685623, 0.802920543839752, 2.4398253816293427)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the variance of the data\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57082809, -0.86849769,  0.52176176, -0.35232913],\n",
       "       [ 0.17545983,  0.69573891,  0.10195772,  0.62988428],\n",
       "       [ 0.21472144, -0.77353652, -0.2369891 , -1.00982055],\n",
       "       [-0.19917833, -0.34555764,  0.73507512,  0.24922768]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4602380800685623, 0.802920543839752, 0.3049781727036678)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "* This is to ensure words don't get context from words generated in the future.\n",
    "\n",
    "* Not required in the encoders, but required int he decoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57082809,        -inf,        -inf,        -inf],\n",
       "       [ 0.17545983,  0.69573891,        -inf,        -inf],\n",
       "       [ 0.21472144, -0.77353652, -0.2369891 ,        -inf],\n",
       "       [-0.19917833, -0.34555764,  0.73507512,  0.24922768]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "* It is usually to convert a vector to a probability distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.37278698, 0.62721302, 0.        , 0.        ],\n",
       "       [0.49781882, 0.18530039, 0.31688079, 0.        ],\n",
       "       [0.16736523, 0.1445751 , 0.42599681, 0.26206287]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58005958,  1.18831246,  0.4010658 , -1.55581048,  1.41179241,\n",
       "         0.5148482 , -0.17225128,  0.41666077],\n",
       "       [ 0.46294173,  0.62117193, -0.40234097, -0.20457387,  0.46161287,\n",
       "         0.19004531,  0.08436345, -0.1596276 ],\n",
       "       [ 0.15605259,  0.70630258,  0.53436808, -0.62667827,  0.59117397,\n",
       "         0.43796768, -0.49917541, -0.26127964],\n",
       "       [-0.25972492,  0.44755252,  0.4701361 ,  0.18543861,  0.74603991,\n",
       "         0.46306653, -0.84643746, -0.56355693]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58005958,  1.18831246,  0.4010658 , -1.55581048,  1.41179241,\n",
       "         0.5148482 , -0.17225128,  0.41666077],\n",
       "       [ 0.3933322 ,  0.28408931, -0.87984952,  0.59853989, -0.10313076,\n",
       "        -0.00300281,  0.23688361, -0.50214727],\n",
       "       [-0.64881368,  0.19596142,  1.57076871,  0.11652266, -0.29201105,\n",
       "         0.57505215, -1.44319289, -1.18547145],\n",
       "       [-0.52384454,  0.4736236 , -0.53012659,  1.18160556,  2.4767373 ,\n",
       "         0.50507957, -0.90459399, -0.21249256]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled * mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.15903265 -0.03436588 -0.54091411 -0.04183514  1.0294876  -1.38443887\n",
      "  -0.20643329  0.63079036]\n",
      " [ 0.28384774 -0.99402038  0.19379103  0.07773752 -0.54139472  0.61485768\n",
      "  -0.66874001 -0.61946928]\n",
      " [-0.4069266   0.21494398 -1.1802294  -0.75233679  0.07718897 -1.41885616\n",
      "   0.21152508  0.7129345 ]\n",
      " [ 0.1134854   0.8669609   0.64626725  0.42897377  0.56820766  0.60987589\n",
      "  -0.0463874   0.50168931]]\n",
      "K\n",
      " [[ 0.45504661 -0.32470325 -0.69371571 -0.29226108  0.61103539 -0.04622356\n",
      "  -0.63832225 -0.2160977 ]\n",
      " [-0.61339223  0.52199931  0.0452046   0.16846916 -0.05370172  0.0053866\n",
      "  -1.15041188 -2.96587082]\n",
      " [ 0.25472673 -0.12084354  0.78193255 -0.32265847  0.87753825  0.55725366\n",
      "  -1.7554051   1.73041697]\n",
      " [ 0.94920364  0.30000482  0.87418704  1.02043214 -1.29928868  0.21313945\n",
      "  -0.90408828 -0.19782976]]\n",
      "V\n",
      " [[ 0.58005958  1.18831246  0.4010658  -1.55581048  1.41179241  0.5148482\n",
      "  -0.17225128  0.41666077]\n",
      " [ 0.3933322   0.28408931 -0.87984952  0.59853989 -0.10313076 -0.00300281\n",
      "   0.23688361 -0.50214727]\n",
      " [-0.64881368  0.19596142  1.57076871  0.11652266 -0.29201105  0.57505215\n",
      "  -1.44319289 -1.18547145]\n",
      " [-0.52384454  0.4736236  -0.53012659  1.18160556  2.4767373   0.50507957\n",
      "  -0.90459399 -0.21249256]]\n",
      "New V\n",
      " [[-0.05897515  0.63035681  0.57121208 -0.32227453  0.8092997   0.48804119\n",
      "  -0.7150834  -0.3539638 ]\n",
      " [-0.03589619  0.50019296 -0.08770449  0.27392867  0.93861094  0.35468821\n",
      "  -0.48961669 -0.35944318]\n",
      " [ 0.06927603  0.67660531  0.3985046  -0.39588356  0.83183201  0.4465333\n",
      "  -0.55091976 -0.25505285]\n",
      " [-0.25972492  0.44755252  0.4701361   0.18543861  0.74603991  0.46306653\n",
      "  -0.84643746 -0.56355693]]\n",
      "Attention\n",
      " [[0.38662758 0.09166459 0.36811507 0.15359276]\n",
      " [0.19279347 0.32437445 0.179131   0.30370108]\n",
      " [0.43428126 0.16165015 0.27643669 0.1276319 ]\n",
      " [0.16736523 0.1445751  0.42599681 0.26206287]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q,k,v,mask=None)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k) \n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.15903265 -0.03436588 -0.54091411 -0.04183514  1.0294876  -1.38443887\n",
      "  -0.20643329  0.63079036]\n",
      " [ 0.28384774 -0.99402038  0.19379103  0.07773752 -0.54139472  0.61485768\n",
      "  -0.66874001 -0.61946928]\n",
      " [-0.4069266   0.21494398 -1.1802294  -0.75233679  0.07718897 -1.41885616\n",
      "   0.21152508  0.7129345 ]\n",
      " [ 0.1134854   0.8669609   0.64626725  0.42897377  0.56820766  0.60987589\n",
      "  -0.0463874   0.50168931]]\n",
      "K\n",
      " [[ 0.45504661 -0.32470325 -0.69371571 -0.29226108  0.61103539 -0.04622356\n",
      "  -0.63832225 -0.2160977 ]\n",
      " [-0.61339223  0.52199931  0.0452046   0.16846916 -0.05370172  0.0053866\n",
      "  -1.15041188 -2.96587082]\n",
      " [ 0.25472673 -0.12084354  0.78193255 -0.32265847  0.87753825  0.55725366\n",
      "  -1.7554051   1.73041697]\n",
      " [ 0.94920364  0.30000482  0.87418704  1.02043214 -1.29928868  0.21313945\n",
      "  -0.90408828 -0.19782976]]\n",
      "V\n",
      " [[ 0.58005958  1.18831246  0.4010658  -1.55581048  1.41179241  0.5148482\n",
      "  -0.17225128  0.41666077]\n",
      " [ 0.3933322   0.28408931 -0.87984952  0.59853989 -0.10313076 -0.00300281\n",
      "   0.23688361 -0.50214727]\n",
      " [-0.64881368  0.19596142  1.57076871  0.11652266 -0.29201105  0.57505215\n",
      "  -1.44319289 -1.18547145]\n",
      " [-0.52384454  0.4736236  -0.53012659  1.18160556  2.4767373   0.50507957\n",
      "  -0.90459399 -0.21249256]]\n",
      "New V\n",
      " [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.48669589  0.73620088 -0.23939186 -0.47863529  0.65433082  0.2559227\n",
      "   0.03231617 -0.04274325]\n",
      " [        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.04981661  0.5354967   0.1404646   0.08521441  0.87334697  0.39799428\n",
      "  -0.57078864 -0.37086263]]\n",
      "Attention\n",
      " [[0.    nan 0.    nan]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   0.   0.    nan]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohan krishnan\\AppData\\Local\\Temp\\ipykernel_11872\\482890027.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q,k,v,mask)\n",
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k) \n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
